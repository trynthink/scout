{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d09fc80b-0d69-421b-a88f-7a744715257d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "residential summer \n",
      "   timestamp_hour state         total\n",
      "0      2018-07-21    AL  1.194938e+08\n",
      "1      2018-06-28    AR  7.565772e+07\n",
      "2      2018-07-25    AZ  2.145976e+08\n",
      "3      2018-07-07    CA  4.298707e+08\n",
      "4      2018-06-28    CO  6.640632e+07\n",
      "5      2018-08-29    CT  4.079587e+07\n",
      "6      2018-07-03    DC  7.807121e+06\n",
      "7      2018-08-29    DE  1.867462e+07\n",
      "8      2018-08-05    FL  4.177733e+08\n",
      "9      2018-06-20    GA  2.141357e+08\n",
      "10     2018-06-17    IA  5.536032e+07\n",
      "11     2018-08-10    ID  2.307976e+07\n",
      "12     2018-06-30    IL  2.119587e+08\n",
      "13     2018-06-18    IN  1.221901e+08\n",
      "14     2018-06-29    KS  7.346131e+07\n",
      "15     2018-07-05    KY  1.014411e+08\n",
      "16     2018-07-22    LA  1.185602e+08\n",
      "17     2018-08-29    MA  6.980715e+07\n",
      "18     2018-07-03    MD  1.085559e+08\n",
      "19     2018-07-05    ME  1.290075e+07\n",
      "20     2018-06-30    MI  1.509794e+08\n",
      "21     2018-06-29    MN  7.470778e+07\n",
      "22     2018-06-30    MO  1.423811e+08\n",
      "23     2018-06-28    MS  7.427273e+07\n",
      "24     2018-08-11    MT  1.175389e+07\n",
      "25     2018-07-03    NC  2.261575e+08\n",
      "26     2018-08-12    ND  1.117023e+07\n",
      "27     2018-06-16    NE  3.815167e+07\n",
      "28     2018-08-29    NH  1.488387e+07\n",
      "29     2018-08-29    NJ  1.317964e+08\n",
      "30     2018-07-22    NM  3.729086e+07\n",
      "31     2018-07-26    NV  7.464503e+07\n",
      "32     2018-08-29    NY  1.959065e+08\n",
      "33     2018-06-18    OH  1.931012e+08\n",
      "34     2018-07-20    OK  1.320855e+08\n",
      "35     2018-07-16    OR  5.338924e+07\n",
      "36     2018-07-01    PA  1.638761e+08\n",
      "37     2018-08-29    RI  1.050871e+07\n",
      "38     2018-06-20    SC  1.165735e+08\n",
      "39     2018-07-08    SD  1.318172e+07\n",
      "40     2018-07-04    TN  1.574774e+08\n",
      "41     2018-07-22    TX  6.810490e+08\n",
      "42     2018-07-07    UT  3.740635e+07\n",
      "43     2018-07-03    VA  1.593176e+08\n",
      "44     2018-07-02    VT  5.791115e+06\n",
      "45     2018-08-09    WA  7.717328e+07\n",
      "46     2018-06-30    WI  8.669986e+07\n",
      "47     2018-07-04    WV  3.626921e+07\n",
      "48     2018-07-07    WY  6.147963e+06\n",
      "{'AL': datetime.date(2018, 7, 21), 'AR': datetime.date(2018, 6, 28), 'AZ': datetime.date(2018, 7, 25), 'CA': datetime.date(2018, 7, 7), 'CO': datetime.date(2018, 6, 28), 'CT': datetime.date(2018, 8, 29), 'DC': datetime.date(2018, 7, 3), 'DE': datetime.date(2018, 8, 29), 'FL': datetime.date(2018, 8, 5), 'GA': datetime.date(2018, 6, 20), 'IA': datetime.date(2018, 6, 17), 'ID': datetime.date(2018, 8, 10), 'IL': datetime.date(2018, 6, 30), 'IN': datetime.date(2018, 6, 18), 'KS': datetime.date(2018, 6, 29), 'KY': datetime.date(2018, 7, 5), 'LA': datetime.date(2018, 7, 22), 'MA': datetime.date(2018, 8, 29), 'MD': datetime.date(2018, 7, 3), 'ME': datetime.date(2018, 7, 5), 'MI': datetime.date(2018, 6, 30), 'MN': datetime.date(2018, 6, 29), 'MO': datetime.date(2018, 6, 30), 'MS': datetime.date(2018, 6, 28), 'MT': datetime.date(2018, 8, 11), 'NC': datetime.date(2018, 7, 3), 'ND': datetime.date(2018, 8, 12), 'NE': datetime.date(2018, 6, 16), 'NH': datetime.date(2018, 8, 29), 'NJ': datetime.date(2018, 8, 29), 'NM': datetime.date(2018, 7, 22), 'NV': datetime.date(2018, 7, 26), 'NY': datetime.date(2018, 8, 29), 'OH': datetime.date(2018, 6, 18), 'OK': datetime.date(2018, 7, 20), 'OR': datetime.date(2018, 7, 16), 'PA': datetime.date(2018, 7, 1), 'RI': datetime.date(2018, 8, 29), 'SC': datetime.date(2018, 6, 20), 'SD': datetime.date(2018, 7, 8), 'TN': datetime.date(2018, 7, 4), 'TX': datetime.date(2018, 7, 22), 'UT': datetime.date(2018, 7, 7), 'VA': datetime.date(2018, 7, 3), 'VT': datetime.date(2018, 7, 2), 'WA': datetime.date(2018, 8, 9), 'WI': datetime.date(2018, 6, 30), 'WV': datetime.date(2018, 7, 4), 'WY': datetime.date(2018, 7, 7)}\n",
      "residential winter \n",
      "   timestamp_hour state         total\n",
      "0      2018-01-17    AL  2.219757e+08\n",
      "1      2018-01-02    AR  1.292460e+08\n",
      "2      2018-12-31    AZ  1.075629e+08\n",
      "3      2018-02-24    CA  2.872615e+08\n",
      "4      2018-02-20    CO  1.083719e+08\n",
      "5      2018-01-07    CT  5.717333e+07\n",
      "6      2018-01-05    DC  1.235838e+07\n",
      "7      2018-01-06    DE  3.039914e+07\n",
      "8      2018-01-04    FL  5.228931e+08\n",
      "9      2018-01-02    GA  3.021318e+08\n",
      "10     2018-01-01    IA  1.093726e+08\n",
      "11     2018-02-20    ID  4.431775e+07\n",
      "12     2018-01-02    IL  2.772386e+08\n",
      "13     2018-01-02    IN  2.575170e+08\n",
      "14     2018-01-16    KS  8.961154e+07\n",
      "15     2018-01-02    KY  2.342373e+08\n",
      "16     2018-01-17    LA  1.961402e+08\n",
      "17     2018-01-06    MA  1.018602e+08\n",
      "18     2018-01-06    MD  2.066204e+08\n",
      "19     2018-01-06    ME  2.164711e+07\n",
      "20     2018-01-05    MI  1.848517e+08\n",
      "21     2018-01-05    MN  1.370263e+08\n",
      "22     2018-01-16    MO  2.755301e+08\n",
      "23     2018-01-17    MS  1.250823e+08\n",
      "24     2018-02-20    MT  3.095630e+07\n",
      "25     2018-01-07    NC  4.437845e+08\n",
      "26     2018-01-15    ND  3.917354e+07\n",
      "27     2018-01-01    NE  8.355220e+07\n",
      "28     2018-01-06    NH  1.995777e+07\n",
      "29     2018-01-06    NJ  1.214302e+08\n",
      "30     2018-12-29    NM  3.433293e+07\n",
      "31     2018-02-24    NV  3.394005e+07\n",
      "32     2018-01-06    NY  2.680643e+08\n",
      "33     2018-01-02    OH  3.478488e+08\n",
      "34     2018-01-16    OK  1.461059e+08\n",
      "35     2018-02-23    OR  9.796710e+07\n",
      "36     2018-01-06    PA  3.036369e+08\n",
      "37     2018-01-06    RI  1.398843e+07\n",
      "38     2018-01-03    SC  1.972728e+08\n",
      "39     2018-01-01    SD  3.679687e+07\n",
      "40     2018-01-17    TN  3.533309e+08\n",
      "41     2018-01-02    TX  8.758995e+08\n",
      "42     2018-02-24    UT  3.177943e+07\n",
      "43     2018-01-06    VA  3.762711e+08\n",
      "44     2018-01-06    VT  8.741377e+06\n",
      "45     2018-02-23    WA  2.077852e+08\n",
      "46     2018-01-01    WI  1.179117e+08\n",
      "47     2018-01-06    WV  9.420083e+07\n",
      "48     2018-02-20    WY  1.574447e+07\n",
      "{'AL': datetime.date(2018, 1, 17), 'AR': datetime.date(2018, 1, 2), 'AZ': datetime.date(2018, 12, 31), 'CA': datetime.date(2018, 2, 24), 'CO': datetime.date(2018, 2, 20), 'CT': datetime.date(2018, 1, 7), 'DC': datetime.date(2018, 1, 5), 'DE': datetime.date(2018, 1, 6), 'FL': datetime.date(2018, 1, 4), 'GA': datetime.date(2018, 1, 2), 'IA': datetime.date(2018, 1, 1), 'ID': datetime.date(2018, 2, 20), 'IL': datetime.date(2018, 1, 2), 'IN': datetime.date(2018, 1, 2), 'KS': datetime.date(2018, 1, 16), 'KY': datetime.date(2018, 1, 2), 'LA': datetime.date(2018, 1, 17), 'MA': datetime.date(2018, 1, 6), 'MD': datetime.date(2018, 1, 6), 'ME': datetime.date(2018, 1, 6), 'MI': datetime.date(2018, 1, 5), 'MN': datetime.date(2018, 1, 5), 'MO': datetime.date(2018, 1, 16), 'MS': datetime.date(2018, 1, 17), 'MT': datetime.date(2018, 2, 20), 'NC': datetime.date(2018, 1, 7), 'ND': datetime.date(2018, 1, 15), 'NE': datetime.date(2018, 1, 1), 'NH': datetime.date(2018, 1, 6), 'NJ': datetime.date(2018, 1, 6), 'NM': datetime.date(2018, 12, 29), 'NV': datetime.date(2018, 2, 24), 'NY': datetime.date(2018, 1, 6), 'OH': datetime.date(2018, 1, 2), 'OK': datetime.date(2018, 1, 16), 'OR': datetime.date(2018, 2, 23), 'PA': datetime.date(2018, 1, 6), 'RI': datetime.date(2018, 1, 6), 'SC': datetime.date(2018, 1, 3), 'SD': datetime.date(2018, 1, 1), 'TN': datetime.date(2018, 1, 17), 'TX': datetime.date(2018, 1, 2), 'UT': datetime.date(2018, 2, 24), 'VA': datetime.date(2018, 1, 6), 'VT': datetime.date(2018, 1, 6), 'WA': datetime.date(2018, 2, 23), 'WI': datetime.date(2018, 1, 1), 'WV': datetime.date(2018, 1, 6), 'WY': datetime.date(2018, 2, 20)}\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.ticker as mticker\n",
    "\n",
    "e_st = 'out.electricity.'\n",
    "r_st = '.energy_consumption'\n",
    "\n",
    "\n",
    "\n",
    "enduse_map = {\n",
    "    'commercial': {\n",
    "        'cooling': [e_st+'cooling'+r_st],\n",
    "        'heating': [e_st+'heat_recovery'+r_st,\n",
    "                    e_st+'heat_rejection'+r_st,\n",
    "                    e_st+'heating'+r_st],\n",
    "        'ventilation': [e_st+'fans'+r_st],\n",
    "        'lighting': [e_st+'interior_lighting'+r_st,\n",
    "                     e_st+'exterior_lighting'+r_st],\n",
    "        'water_heating': [e_st+'water_systems'+r_st],\n",
    "        'refrigeration': [e_st+'refrigeration'+r_st],\n",
    "        'other_mels': [e_st+'interior_equipment'+r_st],\n",
    "        'pumps': [e_st+'pumps'+r_st]\n",
    "    },\n",
    "    'residential': {\n",
    "        'heating': [e_st+'heating'+r_st,\n",
    "                    e_st+'heating_hp_bkup'+r_st],\n",
    "        'cooling': [e_st+'cooling'+r_st],\n",
    "        'water_heating': [e_st+'hot_water'+r_st],\n",
    "        'cooking': [e_st+'range_oven'+r_st],        \n",
    "        'lighting': [e_st+'lighting_interior'+r_st,\n",
    "                     e_st+'lighting_exterior'+r_st,\n",
    "                     e_st+'lighting_garage'+r_st],\n",
    "        'refrigeration': [e_st+'refrigerator'+r_st,\n",
    "                         e_st+'freezer'+r_st],\n",
    "        'ceiling_fan': [e_st+'ceiling_fan'+r_st],\n",
    "        'fans_and_pumps': [e_st+'heating_fans_pumps'+r_st,\n",
    "                          e_st+'cooling_fans_pumps'+r_st,\n",
    "                          e_st+'well_pump'+r_st,\n",
    "                          e_st+'heating_hp_bkup_fa'+r_st,\n",
    "                          e_st+'mech_vent'+r_st],\n",
    "        'clothes_washing': [e_st+'clothes_washer'+r_st],\n",
    "        'drying': [e_st+'clothes_dryer'+r_st],\n",
    "        'dishwasher': [e_st+'dishwasher'+r_st],\n",
    "        'pool_heaters': [e_st+'pool_heater'+r_st],\n",
    "        'pool_pumps': [e_st+'pool_pump'+r_st],\n",
    "        'other': [e_st+'plug_loads'+r_st],\n",
    "        'portable_electric_spas': [e_st+'permanent_spa_heat'+r_st,\n",
    "                                  e_st+'permanent_spa_pump'+r_st]\n",
    "    }}\n",
    "\n",
    "\n",
    "def all_df(df, date):\n",
    "    df = df[df.index.strftime('%m-%d') != date]\n",
    "    return df\n",
    "\n",
    "def get_normalized(df,com_res):\n",
    "    end_uses = list(enduse_map[com_res])\n",
    "    df = df.drop(end_uses, axis=1)\n",
    "    df.columns = df.columns.str.replace('_sqft', '')\n",
    "    return df\n",
    "  \n",
    "#######################################################################################\n",
    "def returnPkDayData(df, dmy):\n",
    "    import numpy as np             \n",
    "    mo = dmy.month\n",
    "    dy = dmy.day\n",
    "    # print(mo,' ',dy)\n",
    "    d_st = datetime.datetime(2018, mo, dy, 1) - pd.Timedelta(hours=0)\n",
    "    d_end = datetime.datetime(2018, mo, dy, 23)\n",
    "    df = df.loc[d_st:d_end]\n",
    "    # df = df[df.index.dayofweek < 5]\n",
    "    # df['dayofweek'] = df.index.dayofweek\n",
    "    # for dt in holidays:\n",
    "    #     df = removeDates(df,dt)\n",
    "    df = df.groupby([df.index.hour]).mean()\n",
    "    df.index = np.arange(1,1+len(df.index))\n",
    "    return df\n",
    "\n",
    "def plot_hourlyloads_byreg(com_res, season, version):\n",
    "    df = pd.read_csv(f'{com_res}_{reg}.csv')\n",
    "    if version == 'normalized':\n",
    "        df = get_normalized(df,com_res)\n",
    "    df = df.set_index('timestamp_hour')\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    # end_uses = [ele for ele in list(df.columns) if ele not in ['timestamp',reg,'building_type']]\n",
    "    end_uses = list(enduse_map[com_res])\n",
    "    i_regions = df[reg].unique()\n",
    "    building_types = df['building_type'].unique()\n",
    "    print(com_res,' ', season)\n",
    "    pkday_i_reg = get_peakdays_i_reg(df, end_uses, season)\n",
    "    \n",
    "    \n",
    "    if com_res == 'commercial':\n",
    "        nrows = 13\n",
    "        ncols = 10\n",
    "    else:\n",
    "        nrows = 13\n",
    "        ncols = 6\n",
    "    \n",
    "    i = j = 0\n",
    "    fig, ax = plt.subplots(nrows, ncols, figsize=(ncols*4+4, nrows*3.5), sharex=True, sharey=False)\n",
    "    for i_reg in i_regions:\n",
    "        for bt in building_types:\n",
    "            for eu in end_uses:\n",
    "                data = df[(df[reg]==i_reg) & (df['building_type']==bt)]\n",
    "                data = returnPkDayData(data, pkday_i_reg[i_reg])\n",
    "                ax[i, j].plot(data[eu], label=eu)\n",
    "                ax[i, j].set_title(f'{i_reg}\\nbt={bt}')\n",
    "                if i == nrows - 1:\n",
    "                    ax[i, j].set_xlabel(\"Hour\")\n",
    "                if j == 0:\n",
    "                    ax[i, j].set_ylabel(\"Loads [kWh]\")\n",
    "            if i==0 and j == ncols-1: ax[i,j].legend(fontsize=13, bbox_to_anchor=(1.5, 1.01), loc=1)\n",
    "            j = j + 1\n",
    "            if j > ncols-1:\n",
    "                i = i + 1\n",
    "                j = 0\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'./figures/hourlyloads/{com_res[:3]}_{season}_peakday_{reg}.png',dpi=100, bbox_inches='tight')\n",
    "\n",
    "def diagnostic_print(df):\n",
    "    import numpy as np\n",
    "    df = df.set_index('timestamp_hour')\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    d_st = datetime.datetime(2018, 1, 5, 1) - pd.Timedelta(hours=0)\n",
    "    d_end = datetime.datetime(2018, 1, 5, 23)\n",
    "    i_reg = 'TRE'\n",
    "    eu = 'heating'\n",
    "    df_bt = pd.DataFrame()\n",
    "    for bt in df['building_type'].unique():\n",
    "        dat = df[(df[reg]=='BASN') & (df['building_type']==bt)]\n",
    "        dat = dat.loc[d_st:d_end]\n",
    "        dat = dat.groupby([dat.index.hour]).mean()\n",
    "        dat.index = np.arange(1,1+len(dat.index))\n",
    "        dat.columns = [bt +'_'+ column_name for column_name in dat.columns]\n",
    "        df_bt = pd.concat([df_bt, dat], axis=1)\n",
    "    df_bt_filtered = df_bt.filter(like=eu)\n",
    "    df_bt_filtered.to_csv(f'diagnostics/diag_{reg}_{eu}.csv', index=False)\n",
    "    print(f'{i_reg}')\n",
    "    print(df_bt_filtered)\n",
    "    return df_bt\n",
    "\n",
    "def get_peakdays_dailytotal_i_reg(data, varis, season):\n",
    "    import numpy as np\n",
    "    # data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "    # data.set_index('timestamp', inplace=True)    \n",
    "    unique_i_reg = data[reg].unique()\n",
    "    data['total'] = data[varis].sum(axis=1)\n",
    "    daily_totals = data.groupby(reg).resample('D')['total'].sum()\n",
    "    daily_totals = daily_totals.reset_index()\n",
    "    filtered_data = daily_totals\n",
    "    if season == 'summer':\n",
    "        filtered_data = daily_totals[(daily_totals['timestamp_hour'].dt.month >= 5) & (daily_totals['timestamp_hour'].dt.month <= 8)]\n",
    "    elif season == 'winter':\n",
    "        filtered_data = daily_totals[((daily_totals['timestamp_hour'].dt.month >= 11) | (daily_totals['timestamp_hour'].dt.month <= 2))]\n",
    "\n",
    "    peak_days = filtered_data.loc[filtered_data.groupby(reg)['total'].idxmax()]\n",
    "    peak_days.drop(columns='total')\n",
    "    \n",
    "    # print(peak_days)\n",
    "    peak_days_dict = dict(zip(peak_days[reg], peak_days['timestamp_hour']))\n",
    "    return peak_days_dict\n",
    "\n",
    "def get_peakdays_i_reg(data, com_res, season):\n",
    "    import numpy as np\n",
    "    # data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "    # data.set_index('timestamp', inplace=True)\n",
    "    data = data.set_index('timestamp_hour')\n",
    "    data.index = pd.to_datetime(data.index)\n",
    "    \n",
    "    unique_i_reg = data[reg].unique()\n",
    "    end_uses = list(enduse_map[com_res])\n",
    "    data['total'] = data[end_uses].sum(axis=1)\n",
    "    if season == 'summer':\n",
    "        data = data[(data.index.month >= 5) & (data.index.month <= 8)]\n",
    "    elif season == 'winter':\n",
    "        data = data[((data.index.month >= 11) | (data.index.month <= 2))]\n",
    "    \n",
    "    data['timestamp_hour'] = data.index.date\n",
    "    data = data[[reg, 'timestamp_hour', 'total']].reset_index(drop=True)    \n",
    "    data = data.groupby(['timestamp_hour',reg]).sum()\n",
    "    df_sorted = data.sort_values([reg, 'total'], ascending=[True, False])\n",
    "    peak_days_df = df_sorted.groupby(reg).head(1)\n",
    "    peak_days_df = peak_days_df.reset_index()\n",
    "    peak_days_dict = dict(zip(peak_days_df[reg], peak_days_df['timestamp_hour']))\n",
    "    print(peak_days_df)\n",
    "    print(peak_days_dict)\n",
    "    return peak_days_dict\n",
    "\n",
    "\n",
    "def plot_hourlyloads_eu(com_res, season, version):\n",
    "    if version == 'normalized':\n",
    "        df = pd.read_csv(f\"csv/normalized/{com_res}_{reg}.csv\")\n",
    "    else:\n",
    "        df = pd.read_csv(f\"csv/{com_res}_{reg}.csv\")\n",
    "    if com_res == \"commercial\":\n",
    "        df = df.loc[df['building_type'].isin(['LargeHotel', 'LargeOffice','MediumOffice','RetailStandalone','Warehouse'])]\n",
    "    elif com_res == \"residential\":\n",
    "        df = df.loc[df['building_type'].isin([\"Multi-Family with 5+ Units\", \"Mobile Home\", \"Single-Family Detached\"])]\n",
    "    print(f'{com_res} {season} {version}')\n",
    "    df_pkdy = df.copy()\n",
    "    pkday_i_reg = get_peakdays_i_reg(df_pkdy, com_res, season)\n",
    "\n",
    "    # print(df[reg].unique())\n",
    "    # diagnostic_print(df) \n",
    "    if version == 'normalized':\n",
    "        df = get_normalized(df,com_res)\n",
    "        \n",
    "    df = df.set_index('timestamp_hour')\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "\n",
    "    end_uses = list(enduse_map[com_res])\n",
    "    i_regions = df[reg].unique()\n",
    "    building_types = df['building_type'].unique()\n",
    "    \n",
    "    if com_res == 'commercial':\n",
    "        nrows = 8\n",
    "        ncols = 5\n",
    "    else:\n",
    "        nrows = 15\n",
    "        ncols = 3\n",
    "    \n",
    "    i = j = 0\n",
    "    fig, ax = plt.subplots(nrows, ncols, figsize=(ncols*5, nrows*3.5), sharex=True, sharey=False)\n",
    "    for eu in end_uses:\n",
    "        for bt in building_types:\n",
    "            for i_reg in i_regions:\n",
    "                dat = df.copy()\n",
    "                data = dat[(dat[reg]==i_reg) & (dat['building_type']==bt)]\n",
    "                data = data.drop([reg, 'building_type'], axis=1)\n",
    "                \n",
    "                data = returnPkDayData(data, pkday_i_reg[i_reg])\n",
    "                ax[i, j].plot(data[eu], label=i_reg)\n",
    "        \n",
    "            ax[i, j].set_title(f'eu={eu}\\nbt={bt}')\n",
    "            if i == nrows - 1:\n",
    "                ax[i, j].set_xlabel(\"Hour\")\n",
    "            \n",
    "            wh = 'kWh'\n",
    "            den = ''\n",
    "            ymin, ymax = ax[i, j].get_ylim()\n",
    "            if version == 'normalized':\n",
    "                if com_res == 'commercial':\n",
    "                    den = \"/ft\\u00b2\"\n",
    "                    ax[i,j].yaxis.set_major_formatter(mticker.FuncFormatter(lambda xi, pos: f'{xi*1000:.3f}'.rstrip('0').rstrip('.')))\n",
    "                elif com_res == 'residential':\n",
    "                    den = \"/housing units\"\n",
    "                    ax[i,j].yaxis.set_major_formatter(mticker.FuncFormatter(lambda xy, pos: f'{xy:.3f}'.rstrip('0').rstrip('.')))\n",
    "            else:\n",
    "                if ymax > 998:\n",
    "                    wh = 'MWh'\n",
    "                    # ax[i,j].yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: f'{x/1000:.0f}'))\n",
    "                    ax[i,j].yaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: f'{x/1000}'.rstrip('0').rstrip('.')))\n",
    "                else:\n",
    "                    ax[i,j].yaxis.set_major_formatter(mticker.FuncFormatter(lambda xy, pos: f'{xy:.3f}'.rstrip('0').rstrip('.')))\n",
    "            ylab = f'{wh}{den}'\n",
    "            \n",
    "            # ax[i,j].yaxis.set_label_coords(0.05, 1.03)\n",
    "            # ax[i, j].set_ylabel(f'{ylab}',rotation=0, ha='right', va='center')\n",
    "            \n",
    "            if version == 'normalized':\n",
    "                if j == 0:\n",
    "                    ax[i, j].set_ylabel(f'{ylab}')\n",
    "                    ax[i, j].yaxis.set_label_coords(-0.1, 0.5)\n",
    "            else:\n",
    "                ax[i, j].set_ylabel(f'{ylab}')\n",
    "                ax[i, j].yaxis.set_label_coords(-0.1, 0.5)\n",
    "            \n",
    "            ###########\n",
    "            if i==0 and j == ncols-1: ax[i,j].legend(fontsize=13, bbox_to_anchor=(1.5, 1.01), loc=1)\n",
    "            j = j + 1\n",
    "            if j > ncols-1:\n",
    "                i = i + 1\n",
    "                j = 0\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'./diagnostics/{com_res[:3]}_{season}_{version}.png',dpi=100, bbox_inches='tight')\n",
    "\n",
    "reg = 'state'\n",
    "\n",
    "plot_hourlyloads_eu('commercial', 'annual','')\n",
    "plot_hourlyloads_eu('commercial', 'summer','')\n",
    "plot_hourlyloads_eu('commercial', 'winter','')\n",
    "\n",
    "plot_hourlyloads_eu('residential', 'annual','')\n",
    "plot_hourlyloads_eu('residential', 'summer','')\n",
    "plot_hourlyloads_eu('residential', 'winter','')\n",
    "\n",
    "# plot_hourlyloads_eu('commercial', 'annual','normalized')\n",
    "# plot_hourlyloads_eu('commercial', 'summer','normalized')\n",
    "# plot_hourlyloads_eu('commercial', 'winter','normalized')\n",
    "\n",
    "# plot_hourlyloads_eu('residential', 'annual','normalized')\n",
    "# plot_hourlyloads_eu('residential', 'summer','normalized')\n",
    "# plot_hourlyloads_eu('residential', 'winter','normalized')\n",
    "\n",
    "# plot_hourlyloads_byreg('commercial', 'annual','normalized')\n",
    "# plot_hourlyloads_byreg('commercial', 'summer','normalized')\n",
    "# plot_hourlyloads_byreg('commercial', 'winter','normalized')\n",
    "\n",
    "# plot_hourlyloads_byreg('residential', 'annual','normalized')\n",
    "# plot_hourlyloads_byreg('residential', 'summer','normalized')\n",
    "# plot_hourlyloads_byreg('residential', 'winter','normalized')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6924ce-a000-47ab-820c-5ec18b357ee6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
